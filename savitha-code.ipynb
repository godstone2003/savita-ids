{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5082681,"sourceType":"datasetVersion","datasetId":2951364}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:09.386478Z","iopub.execute_input":"2025-04-05T09:50:09.386799Z","iopub.status.idle":"2025-04-05T09:50:09.413697Z","shell.execute_reply.started":"2025-04-05T09:50:09.386775Z","shell.execute_reply":"2025-04-05T09:50:09.413063Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bot-iot/data_26.csv\n/kaggle/input/bot-iot/data_33.csv\n/kaggle/input/bot-iot/data_49.csv\n/kaggle/input/bot-iot/data_44.csv\n/kaggle/input/bot-iot/data_38.csv\n/kaggle/input/bot-iot/data_22.csv\n/kaggle/input/bot-iot/data_30.csv\n/kaggle/input/bot-iot/data_57.csv\n/kaggle/input/bot-iot/data_46.csv\n/kaggle/input/bot-iot/data_58.csv\n/kaggle/input/bot-iot/data_21.csv\n/kaggle/input/bot-iot/data_59.csv\n/kaggle/input/bot-iot/data_32.csv\n/kaggle/input/bot-iot/data_69.csv\n/kaggle/input/bot-iot/data_19.csv\n/kaggle/input/bot-iot/data_47.csv\n/kaggle/input/bot-iot/data_14.csv\n/kaggle/input/bot-iot/data_9.csv\n/kaggle/input/bot-iot/data_52.csv\n/kaggle/input/bot-iot/data_18.csv\n/kaggle/input/bot-iot/data_53.csv\n/kaggle/input/bot-iot/data_73.csv\n/kaggle/input/bot-iot/data_63.csv\n/kaggle/input/bot-iot/data_70.csv\n/kaggle/input/bot-iot/data_17.csv\n/kaggle/input/bot-iot/data_7.csv\n/kaggle/input/bot-iot/data_67.csv\n/kaggle/input/bot-iot/data_names.csv\n/kaggle/input/bot-iot/data_11.csv\n/kaggle/input/bot-iot/data_36.csv\n/kaggle/input/bot-iot/data_37.csv\n/kaggle/input/bot-iot/data_40.csv\n/kaggle/input/bot-iot/data_66.csv\n/kaggle/input/bot-iot/data_24.csv\n/kaggle/input/bot-iot/data_16.csv\n/kaggle/input/bot-iot/data_28.csv\n/kaggle/input/bot-iot/data_20.csv\n/kaggle/input/bot-iot/data_72.csv\n/kaggle/input/bot-iot/data_31.csv\n/kaggle/input/bot-iot/data_74.csv\n/kaggle/input/bot-iot/data_10.csv\n/kaggle/input/bot-iot/data_6.csv\n/kaggle/input/bot-iot/data_1.csv\n/kaggle/input/bot-iot/data_48.csv\n/kaggle/input/bot-iot/data_56.csv\n/kaggle/input/bot-iot/data_34.csv\n/kaggle/input/bot-iot/data_4.csv\n/kaggle/input/bot-iot/data_60.csv\n/kaggle/input/bot-iot/data_43.csv\n/kaggle/input/bot-iot/data_65.csv\n/kaggle/input/bot-iot/data_2.csv\n/kaggle/input/bot-iot/data_42.csv\n/kaggle/input/bot-iot/data_51.csv\n/kaggle/input/bot-iot/data_12.csv\n/kaggle/input/bot-iot/data_45.csv\n/kaggle/input/bot-iot/data_25.csv\n/kaggle/input/bot-iot/data_64.csv\n/kaggle/input/bot-iot/data_62.csv\n/kaggle/input/bot-iot/data_27.csv\n/kaggle/input/bot-iot/data_71.csv\n/kaggle/input/bot-iot/data_8.csv\n/kaggle/input/bot-iot/data_39.csv\n/kaggle/input/bot-iot/data_23.csv\n/kaggle/input/bot-iot/data_35.csv\n/kaggle/input/bot-iot/data_54.csv\n/kaggle/input/bot-iot/data_41.csv\n/kaggle/input/bot-iot/data_50.csv\n/kaggle/input/bot-iot/data_5.csv\n/kaggle/input/bot-iot/data_13.csv\n/kaggle/input/bot-iot/data_68.csv\n/kaggle/input/bot-iot/data_3.csv\n/kaggle/input/bot-iot/data_55.csv\n/kaggle/input/bot-iot/data_15.csv\n/kaggle/input/bot-iot/data_61.csv\n/kaggle/input/bot-iot/data_29.csv\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"# check the files exist.. also see the sample format for reference..","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nsample_file = \"/kaggle/input/bot-iot/data_11.csv\"\nsample_df = pd.read_csv(sample_file)\nprint(sample_df.memory_usage().sum() / (1024**3), \"GB\")  # Size in GB\nprint(sample_df.shape)  # Rows, columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:09.414804Z","iopub.execute_input":"2025-04-05T09:50:09.415101Z","iopub.status.idle":"2025-04-05T09:50:12.416988Z","shell.execute_reply.started":"2025-04-05T09:50:09.415069Z","shell.execute_reply":"2025-04-05T09:50:12.416253Z"}},"outputs":[{"name":"stdout","text":"0.26077044010162354 GB\n(1000000, 35)\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"# Using the dask based dataframe and parquent file for ensuring the size efficiency of the file and to be able to handle the low computation and ram of a avgerage system..  **Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval.**  total dataset size is ~15 gb converted into ~1Gb(70%) of the dataset   ","metadata":{}},{"cell_type":"markdown","source":"# dask approach","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\nimport pandas as pd\n\n# Specify only the columns we need to reduce memory footprint\nselected_columns = [\n    'pkts', 'bytes', 'sbytes', 'dbytes', 'dur', 'rate', 'mean', 'stddev', 'sum', \n    'proto', 'state', 'stime', 'category'  # Ensure 'category' is included\n]\n\n# Load all CSV files with Dask\ndf = dd.read_csv(\"/kaggle/input/bot-iot/*.csv\", usecols=selected_columns, dtype={'category': 'object'})\n\n# Basic preprocessing (lazy evaluation)\ndf = df.dropna()  # Drops NaN rows when computed\n\n# Encode categorical features (excluding 'category' to retain original class labels)\ndf['proto'] = df['proto'].map_partitions(lambda x: pd.Categorical(x).codes)\ndf['state'] = df['state'].map_partitions(lambda x: pd.Categorical(x).codes)\n\n# Compute class distribution before encoding 'category'\nclass_counts = df['category'].value_counts().compute()\n\n# Print class counts\nprint(\"Class Distribution:\\n\", class_counts)\n\n# Encode 'category' after checking distributions\ndf['category'] = df['category'].map_partitions(lambda x: pd.Categorical(x, categories=class_counts.index).codes)\n\n# Print preview of processed dataframe\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:12.418784Z","iopub.execute_input":"2025-04-05T09:50:12.419001Z","iopub.status.idle":"2025-04-05T09:50:16.790076Z","shell.execute_reply.started":"2025-04-05T09:50:12.418982Z","shell.execute_reply":"2025-04-05T09:50:16.751822Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-56d40c35aa9d>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Compute class distribution before encoding 'category'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Print class counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask_expr/_collection.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, fuse, concatenate, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDaskMethodsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mshorten_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":71},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Define the path to CSV files\n# csv_path = \"/kaggle/input/bot-iot/*.csv\"\n\n# Read only the 'category' column, ensuring it's treated as a string\n# df = dd.read_csv(csv_path, usecols=['category'], dtype={'category': 'object'})\n\n# Compute category distribution\n\n\n# Display class counts\nprint(df['category'].value_counts().compute())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.791246Z","iopub.status.idle":"2025-04-05T09:50:16.791682Z","shell.execute_reply":"2025-04-05T09:50:16.791518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# type casting to lower datatypes to make use of less memory(batch efficiency)","metadata":{}},{"cell_type":"code","source":"\ndf = df.astype({\n    'pkts': 'int16', 'bytes': 'int32', 'sbytes': 'int32', 'dbytes': 'int32',\n    'dur': 'float32', 'rate': 'float32', 'mean': 'float32', 'stddev': 'float32', 'sum': 'float32',\n    'proto': 'int8', 'state': 'int8', 'stime': 'float32', 'category': 'int8'\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.792225Z","iopub.status.idle":"2025-04-05T09:50:16.792572Z","shell.execute_reply":"2025-04-05T09:50:16.792440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# down sizing the dataset using 70% of the dataset to ensure the organility","metadata":{}},{"cell_type":"code","source":"# For Dask\ndf = df.sample(frac=0.7, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.794977Z","iopub.status.idle":"2025-04-05T09:50:16.795324Z","shell.execute_reply":"2025-04-05T09:50:16.795171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_parquet(\"/kaggle/working/processed_data.parquet\")  # Dask\ndf = dd.read_parquet(\"/kaggle/working/processed_data.parquet\")  # Reload later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.795817Z","iopub.status.idle":"2025-04-05T09:50:16.796113Z","shell.execute_reply":"2025-04-05T09:50:16.795983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Info and checking for null values","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Load the Parquet file (if not already loaded)\ndf = dd.read_parquet(\"/kaggle/working/processed_data.parquet\")\n\n# Get basic information about the dataset\nprint(df.info())  # Check columns and data types\nprint(df.memory_usage(deep=True).compute())  # Check memory usage\nprint(df.head())  # Preview first few rows\n\n# Check for missing values\nmissing_values = df.isnull().sum().compute()\nprint(\"Missing Values:\\n\", missing_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.796591Z","iopub.status.idle":"2025-04-05T09:50:16.796891Z","shell.execute_reply":"2025-04-05T09:50:16.796761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# class counts","metadata":{}},{"cell_type":"code","source":"# Compute class distribution\nclass_counts = df['category'].value_counts().compute()\nprint(\"Class Distribution:\\n\", class_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.798643Z","iopub.status.idle":"2025-04-05T09:50:16.799639Z","shell.execute_reply":"2025-04-05T09:50:16.799470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# numeric cols Distributions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select a few numeric columns to visualize\nnumeric_columns = ['pkts', 'bytes', 'sbytes', 'dbytes', 'dur', 'rate']\n\n# Convert Dask DF to Pandas (for small samples)\nsample_df = df[numeric_columns].sample(frac=0.001, random_state=42).compute()\n\n# Plot histograms\nsample_df.hist(figsize=(12, 8), bins=50)\nplt.suptitle(\"Feature Distributions\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.806184Z","iopub.status.idle":"2025-04-05T09:50:16.806562Z","shell.execute_reply":"2025-04-05T09:50:16.806382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Detect and Handle Skewed Features","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\n\n# Check skewness of numeric features\nskewness = sample_df.skew()\nprint(\"Feature Skewness:\\n\", skewness)\n\n# Apply log transformation if necessary\nfor col in skewness[abs(skewness) > 1].index:\n    sample_df[col] = sample_df[col].apply(lambda x: np.log1p(x))  # Log transformation\n\n# Plot transformed distributions\nsample_df.hist(figsize=(12, 8), bins=50)\nplt.suptitle(\"Log-Transformed Feature Distributions\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.807761Z","iopub.status.idle":"2025-04-05T09:50:16.808089Z","shell.execute_reply":"2025-04-05T09:50:16.807951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Class Distribution (Bar Chart)","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute class distribution\nclass_counts = df['category'].value_counts().compute()\n\n# Plot class distribution\nplt.figure(figsize=(10, 5))\nsns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\nplt.xlabel(\"Attack Categories\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Class Distribution in Bot-IoT Dataset\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.809408Z","iopub.status.idle":"2025-04-05T09:50:16.809741Z","shell.execute_reply":"2025-04-05T09:50:16.809598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Distributions (Histograms)","metadata":{}},{"cell_type":"code","source":"# Select numeric columns\nnumeric_columns = ['pkts', 'bytes', 'sbytes', 'dbytes', 'dur', 'rate']\n\n# Compute a small sample for visualization\nsample_df = df[numeric_columns].sample(frac=0.001, random_state=42).compute()\n\n# Plot histograms\nsample_df.hist(figsize=(12, 8), bins=50, edgecolor='black', alpha=0.7)\nplt.suptitle(\"Feature Distributions in Bot-IoT Dataset\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.814233Z","iopub.status.idle":"2025-04-05T09:50:16.814610Z","shell.execute_reply":"2025-04-05T09:50:16.814464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Boxplots to Identify Outliers","metadata":{}},{"cell_type":"code","source":"# Plot boxplots for numeric features\nplt.figure(figsize=(12, 6))\nsample_df.boxplot(column=numeric_columns, vert=False)\nplt.title(\"Feature Boxplots - Detecting Outliers\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.815326Z","iopub.status.idle":"2025-04-05T09:50:16.815665Z","shell.execute_reply":"2025-04-05T09:50:16.815525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pairplot for Feature Relationships","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\n# Take a small sample for visualization\nsample_df = sample_df.sample(1000, random_state=42)  # Reduce size for visualization\n\n# Pairplot\nsns.pairplot(sample_df, diag_kind='kde')\nplt.suptitle(\"Pairplot of Selected Features\", y=1.02)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.816132Z","iopub.status.idle":"2025-04-05T09:50:16.816412Z","shell.execute_reply":"2025-04-05T09:50:16.816287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Distribution of Packet Durations Across Attack Types\n","metadata":{}},{"cell_type":"code","source":"# Convert to Pandas before plotting\ndf_sample = df[['category', 'dur']].sample(frac=0.001, random_state=42).compute()\n\n# Now, plot using Seaborn\nplt.figure(figsize=(12, 6))\nsns.boxplot(x=df_sample['category'], y=df_sample['dur'], palette=\"coolwarm\")\nplt.xlabel(\"Attack Category\")\nplt.ylabel(\"Packet Duration\")\nplt.title(\"Distribution of Packet Durations Across Attack Types\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.817209Z","iopub.status.idle":"2025-04-05T09:50:16.817564Z","shell.execute_reply":"2025-04-05T09:50:16.817417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Protocol Usage in Different Attacks","metadata":{}},{"cell_type":"code","source":"# Sample data for better visualization\nsample_df = df.sample(frac=0.001, random_state=42).compute()\n\n# Countplot for protocol usage\nplt.figure(figsize=(12, 6))\nsns.countplot(x='proto', hue='category', data=sample_df, palette=\"coolwarm\")\nplt.xlabel(\"Protocol\")\nplt.ylabel(\"Count\")\nplt.title(\"Protocol Usage in Different Attack Categories\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Attack Type\", bbox_to_anchor=(1, 1))\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.818337Z","iopub.status.idle":"2025-04-05T09:50:16.818680Z","shell.execute_reply":"2025-04-05T09:50:16.818541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check the class distribution","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Load the Parquet file using Dask\ndf = dd.read_parquet(\"/kaggle/working/processed_data.parquet\")\n\n# Compute class distribution\nclass_counts = df['category'].value_counts().compute()\n\n# Print class counts\nprint(class_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.819457Z","iopub.status.idle":"2025-04-05T09:50:16.819702Z","shell.execute_reply":"2025-04-05T09:50:16.819601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Compute Dask DataFrame to pandas (after filtering/sampling)\ndf = df.compute()  # Ensure this fits in ~15 GB post-optimization\n\n# Split features and target\n# X = df.drop(columns=['category']).values\n# y = df['category'].values\n\n# # Convert to tensors\n# X_tensor = torch.FloatTensor(X)\n# y_tensor = torch.LongTensor(y)\n\n# # Create DataLoader\n# dataset = TensorDataset(X_tensor, y_tensor)\n# train_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)  # Use 2 workers for Kaggle CPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.820601Z","iopub.status.idle":"2025-04-05T09:50:16.820936Z","shell.execute_reply":"2025-04-05T09:50:16.820788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check the memory usage make shure we dont crash on ram","metadata":{}},{"cell_type":"code","source":"import psutil\nprint(f\"Memory used: {psutil.virtual_memory().percent}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.821728Z","iopub.status.idle":"2025-04-05T09:50:16.822068Z","shell.execute_reply":"2025-04-05T09:50:16.821920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature extraction using tree based classifer and l1 regularization (since  dealing with high-dimensional datasets, noisy data, and situations where computational efficiency is essential.)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.linear_model import Lasso\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Path to folder containing multiple CSV files\ndata_folder = '/kaggle/input/bot-iot'  # Update with your folder path\n\n# Read a sample from all CSVs to analyze feature importance\nsample_size = 500000  # Adjust based on memory\nall_data = []\n\ndef sample_data(file_path, sample_size):\n    chunk = pd.read_csv(file_path, nrows=sample_size, low_memory=False)\n    return chunk\n\nfor filename in os.listdir(data_folder):\n    if filename.endswith('.csv'):\n        file_path = os.path.join(data_folder, filename)\n        print(f\"Sampling from {filename}...\")\n        df_sample = sample_data(file_path, sample_size // len(os.listdir(data_folder)))\n        all_data.append(df_sample)\n\n# Merge sampled data\ndf = pd.concat(all_data, ignore_index=True)\n\n# Remove unnecessary columns\nnull_cols = ['dco', 'sco', 'doui', 'soui', 'dmac', 'smac']  # Replace with actual null columns\ndf.drop(columns=['pkSeqID'] + null_cols, inplace=True, errors='ignore')\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Detect categorical columns dynamically\ncategorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Convert categorical columns to strings before encoding\nfor col in categorical_cols:\n    df[col] = df[col].astype(str)\n\n# Encode categorical features\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])\n    label_encoders[col] = le\n\n# Define target variable\nX = df.drop(columns=['category'])  # Update 'Label' to actual target column\ny = df['category']\n\n# Correlation Analysis\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), cmap='coolwarm', annot=False)\nplt.title(\"Feature Correlation Matrix\")\nplt.show()\n\n# Mutual Information\nmi_scores = mutual_info_classif(X, y, discrete_features=False)\nmi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 6))\nmi_scores.plot(kind='bar')\nplt.title(\"Mutual Information Scores\")\nplt.show()\n\n# Random Forest Feature Importance\nrf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\nrf.fit(X, y)\nfeature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 6))\nfeature_importances.plot(kind='bar')\nplt.title(\"Random Forest Feature Importance\")\nplt.show()\n\n# Lasso Regression for Feature Selection\nlasso = Lasso(alpha=0.01)\nlasso.fit(X, y)\nlasso_importance = pd.Series(np.abs(lasso.coef_), index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 6))\nlasso_importance.plot(kind='bar')\nplt.title(\"Lasso Regression Feature Importance\")\nplt.show()\n\n# Select the Top 10 Most Important Features\n# top_features = feature_importances.head(10).index.tolist()\n# print(\"Top 10 Selected Features:\", top_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.823962Z","iopub.status.idle":"2025-04-05T09:50:16.824294Z","shell.execute_reply":"2025-04-05T09:50:16.824168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get top 10 features from both methods\nrf_top_features = set(feature_importances.head(10).index.tolist())\nlasso_top_features = set(lasso_importance.head(10).index.tolist())\n\nprint(\"Top 10 Selected Features (Intersection of RF and Lasso):\", rf_top_features)\nprint(\"Top 10 Selected Features (Intersection of RF and Lasso):\", lasso_top_features)\n\n# Find the intersection\ntop_features = rf_top_features.intersection(lasso_top_features)\n\nprint(\"Top 10 Selected Features (Intersection of RF and Lasso):\", top_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.825746Z","iopub.status.idle":"2025-04-05T09:50:16.826009Z","shell.execute_reply":"2025-04-05T09:50:16.825903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# the below feature that are selected are the combination from feature selection process and Domain knowledge ..","metadata":{}},{"cell_type":"code","source":"df = dd.read_parquet(\"/kaggle/working/processed_data.parquet\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.826905Z","iopub.status.idle":"2025-04-05T09:50:16.827151Z","shell.execute_reply":"2025-04-05T09:50:16.827049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# current data preprocessing appraoch.. \n#** intial dataset distributions\n Class 0: ~450,000 samples\nClass 1: 22 samples\nClass 2: ~525,000 samples\nClass 3: ~124 samples\nClass 4: ~25,000 samples\n\nthen test set is defined as follewed:\nthing is \n\nUsing the proportions from the test set:\n\nClass 0: \n(449,939/1,000,000)×4,000,000 ≈ 1,799,756\n\nClass 1: \n(22/1,000,000)×4,000,000 ≈ 88\n\nClass 2: \n(525,176/1,000,000) ×4,000,000 ≈  2,100,704\n\nClass 3: \n(124/1,000,000)×4,000,000 ≈ 496\n\nClass 4: \n(24,739/1,000,000) ×4,000,000 ≈98,956\n\nSO training set  before sampling were\n Class 0: 1,799,756 (~45.0%)\n Class 1: 88 (~0.002%)\n Class 2: 2,100,704 (~52.5%)\n Class 3: 496 (~0.012%)\n Class 4: 98,956 (~2.5%)\n\n step2: under sampling the majority class\n  RandomUnderSampler to undersample the majority classes (Classes 0, 2, and 4) to 50,000 samples each\n  Class Distribution After Undersampling:\n\nClass 0: 50,000 (~49.2%)\nClass 1: 88 (~0.09%)\nClass 2: 50,000 (~49.2%)\nClass 3: 496 (~0.49%)\nClass 4: 50,000 (~49.2%)\nTotal samples: 50,000 + 88 + 50,000 + 496 + 50,000 = 150,584\n\n\nstep3: oversampling with the adasyn due to the (reason:Unlike SMOTE, which generates synthetic samples uniformly across the feature space, ADASYN focuses on generating more synthetic samples for minority instances that are harder to classify correctly, based on their level of difficulty in classification)\n\nFinal Class Distribution in Training Set (After ADASYN):\n\nClass 0: 50,000 (~12.5%)\nClass 1: 75,000 (~18.75%)\nClass 2: 50,000 (~12.5%)\nClass 3: 125,000 (~31.25%)\nClass 4: 75,000 (~18.75%)\nTotal samples: 50,000 + 75,000 + 50,000 + 125,000 + 75,000 = 375,000","metadata":{}},{"cell_type":"markdown","source":"# data preprocessing ","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nimport gc\n\n# Step 1: Inspect the 'category' column\nprint(\"Unique values in 'category':\", np.unique(df['category']))\nprint(\"Class distribution before sampling:\\n\", df['category'].value_counts())\n\n# Step 2: Combine with Chebyshev distance\nX_static = np.array(df[['pkts', 'bytes', 'sbytes', 'dbytes', 'rate', 'mean', 'stddev', 'sum', 'proto', 'state']].values, dtype=np.float32)\nX_temporal = np.array(df[['stime', 'dur']].values, dtype=np.float32)\nX_static_mean = X_static.mean(axis=1)\nX_static_std = X_static.std(axis=1)\nX_static_reduced = np.vstack([X_static_mean, X_static_std]).T\nchebyshev_dist = np.max(np.abs(X_static_reduced - X_temporal), axis=1)\nX_combined = np.hstack((X_static, chebyshev_dist.reshape(-1, 1)))\ny = np.array(df['category'].values, dtype=np.int64)\n\n# Free memory\ndel df, X_static, X_temporal, X_static_mean, X_static_std, X_static_reduced, chebyshev_dist\ngc.collect()\n\n# Step 3: Sample to reduce size (target ~5M samples total)\nnp.random.seed(42)\nsample_indices = np.random.choice(len(X_combined), size=5_000_000, replace=False)\nX_combined = X_combined[sample_indices]\ny = y[sample_indices]\n\n# Compute class distribution after sampling\nclass_counts_sampled = np.bincount(y)\nprint(\"Class distribution after sampling:\", {i: count for i, count in enumerate(class_counts_sampled)})\n\n# Step 4: Train-test split BEFORE sampling\nX_train, X_test, y_train, y_test = train_test_split(\n    X_combined, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Free memory\ndel X_combined, y\ngc.collect()\n\n# Compute class distribution in training set\nclass_counts_train = np.bincount(y_train)\nprint(\"Class distribution in training set:\", {i: count for i, count in enumerate(class_counts_train)})\n\n# Step 5: Hybrid Sampling to Balance the Training Set\n# 5.1: Undersample the majority classes (Classes 0, 2, and 4)\nundersampler = RandomUnderSampler(\n    sampling_strategy={\n        0: 50_000,  # Target 50K for Class 0\n        2: 50_000,  # Target 50K for Class 2\n        4: 50_000   # Target 50K for Class 4\n    },\n    random_state=42\n)\nX_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n\n# Free memory\ndel X_train, y_train\ngc.collect()\n\n# Compute class distribution after undersampling\nclass_counts_under = np.bincount(y_train_under)\nprint(\"Class distribution after undersampling:\", {i: count for i, count in enumerate(class_counts_under)})\n\n# 5.2: Apply ADASYN to oversample the minority classes (Classes 1, 3)\nadasyn = ADASYN(\n    random_state=42,\n    sampling_strategy={\n        0: 50_000,  # Already at 50K\n        1: 75_000,  # Oversample to 75K\n        2: 50_000,  # Already at 50K\n        3: 100_000, # Further reduced oversampling for Class 3\n        4: 75_000   # Already at 75K\n    }\n)\nX_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_under, y_train_under)\n\n# Add random noise to synthetic samples to increase diversity\nnoise = np.random.normal(0, 0.02, X_train_resampled.shape)\nX_train_resampled += noise\n\n# Free memory\ndel X_train_under, y_train_under\ngc.collect()\n\n# Compute class distribution after ADASYN\nclass_counts_resampled = np.bincount(y_train_resampled)\nprint(\"Class distribution after ADASYN:\", {i: count for i, count in enumerate(class_counts_resampled)})\n\n\n# Step 6: Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_resampled)\nX_test_scaled = scaler.transform(X_test)\n\n# Free memory\ndel X_train_resampled, X_test\ngc.collect()\n\n# Step 7: Convert to tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train_resampled, dtype=torch.int64)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n\n# Free memory\ndel X_train_scaled, X_test_scaled, y_train_resampled, y_test\ngc.collect()\n\n# Step 8: Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=4)\n\nprint(\"Data loading complete! Ready for SpinalSAENet training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.828869Z","iopub.status.idle":"2025-04-05T09:50:16.829293Z","shell.execute_reply":"2025-04-05T09:50:16.829132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"joblib.dump(scaler, 'scaler.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.830119Z","iopub.status.idle":"2025-04-05T09:50:16.830469Z","shell.execute_reply":"2025-04-05T09:50:16.830313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import label_binarize\n\n# Define Focal Loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.5, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n        self.alpha = alpha  # Class weights\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none')(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        return focal_loss\n\n# Define SpinalSAENet (unchanged)\nclass SpinalSAENet(nn.Module):\n    def __init__(self, input_dim, hidden_dims=[128, 64], num_classes=5):\n        super(SpinalSAENet, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dims[0]),\n            nn.ReLU(),\n            nn.Dropout(0.6),\n            nn.Linear(hidden_dims[0], hidden_dims[1]),\n            nn.ReLU()\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dims[1], 32),\n            nn.ReLU(),\n            nn.Dropout(0.6),\n            nn.Linear(32, num_classes)\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        output = self.classifier(encoded)\n        return output\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Compute input_dim and num_classes\ninput_dim = train_dataset.tensors[0].shape[1]  # 11\nnum_classes = len(torch.unique(train_dataset.tensors[1]))  # Should be 5\nprint(f\"Input dimension: {input_dim}, Number of classes: {num_classes}\")\n\n# Initialize model\nmodel = SpinalSAENet(input_dim, hidden_dims=[128, 64], num_classes=num_classes)\nmodel = model.to(device)\n\n# Class weights (logarithmic inverse frequency with adjustment for Class 3)\ny_test = test_dataset.tensors[1].numpy()\nclass_counts = np.bincount(y_test)\nprint(\"Class distribution in test set:\", {i: count for i, count in enumerate(class_counts)})\nclass_weights = np.log(1.0 / class_counts + 1e-6)  # Add small constant to avoid log(0)\nclass_weights = class_weights / class_weights.sum() * len(class_counts)\n# Increase weight for Class 3\nclass_weights[3] *= 2.0  # Double the weight for Class 3\nclass_weights = torch.FloatTensor(class_weights).to(device)\n\n# Use Focal Loss\ncriterion_clf = FocalLoss(alpha=class_weights, gamma=2.5)\noptimizer_clf = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_clf, mode='min', factor=0.1, patience=3)\nscaler = torch.cuda.amp.GradScaler()\n\nprint(\"Training Classifier...\")\nbest_val_loss = float('inf')\npatience_counter = 0\npatience = 5  # Early stopping patience\n\nfor epoch in range(20):\n    model.train()\n    train_loss = 0.0\n    for data, target in train_loader:\n        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n        optimizer_clf.zero_grad()\n        with torch.cuda.amp.autocast():\n            output = model(data)\n            loss = criterion_clf(output, target)\n        if torch.isnan(loss):\n            print(\"NaN loss detected in training, skipping batch\")\n            continue\n        scaler.scale(loss).backward()\n        scaler.step(optimizer_clf)\n        scaler.update()\n        train_loss += loss.item()\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    all_preds = []\n    all_targets = []\n    all_scores = []\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                outputs = model(data)\n                loss = criterion_clf(outputs, target)\n            val_loss += loss.item()\n            # Apply threshold adjustment for Class 3\n            probs = torch.softmax(outputs, dim=1)\n            class_3_threshold = 0.7  # Increase threshold for Class 3\n            probs[:, 3] = torch.where(probs[:, 3] >= class_3_threshold, probs[:, 3], torch.tensor(0.0, device=device))\n            _, predicted = torch.max(probs, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            all_scores.extend(outputs.cpu().numpy())  # Raw logits for ROC\n    \n    val_loss /= len(test_loader)\n    accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n    f1_weighted = f1_score(all_targets, all_preds, average='weighted')\n    f1_macro = f1_score(all_targets, all_preds, average='macro')\n    precision, recall, f1_per_class, _ = precision_recall_fscore_support(all_targets, all_preds, average=None, zero_division=0)\n    \n    print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%, F1-Weighted: {f1_weighted:.4f}, F1-Macro: {f1_macro:.4f}\")\n    print(\"Per-class F1-scores:\", {f\"Class {i}\": f1 for i, f1 in enumerate(f1_per_class)})\n    \n    scheduler.step(val_loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"spinal_saenet_best.pth\")\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Validation loss not improving, stopping training.\")\n            break\n\n# Load best model\nmodel.load_state_dict(torch.load(\"spinal_saenet_best.pth\"))\nprint(\"Loaded best model with validation loss:\", best_val_loss)\n\n# Generate confusion matrix and ROC curves\nmodel.eval()\nall_preds = []\nall_targets = []\nall_scores = []\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n        outputs = model(data)\n        # Apply threshold adjustment for Class 3\n        probs = torch.softmax(outputs, dim=1)\n        class_3_threshold = 0.7  # Increase threshold for Class 3\n        probs[:, 3] = torch.where(probs[:, 3] >= class_3_threshold, probs[:, 3], torch.tensor(0.0, device=device))\n        _, predicted = torch.max(probs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(target.cpu().numpy())\n        all_scores.extend(outputs.cpu().numpy())  # Raw logits\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:05:33.486646Z","iopub.execute_input":"2025-04-05T10:05:33.486930Z","iopub.status.idle":"2025-04-05T10:09:49.495789Z","shell.execute_reply.started":"2025-04-05T10:05:33.486909Z","shell.execute_reply":"2025-04-05T10:09:49.494755Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nInput dimension: 11, Number of classes: 5\nClass distribution in test set: {0: 449939, 1: 22, 2: 525176, 3: 124, 4: 24739}\nTraining Classifier...\nEpoch 1, Train Loss: 0.3459, Val Loss: 0.3583, Test Accuracy: 62.38%, F1-Weighted: 0.6179, F1-Macro: 0.7530\nPer-class F1-scores: {'Class 0': 0.6466270810627983, 'Class 1': 1.0, 'Class 2': 0.5754307620062492, 'Class 3': 0.544642857142857, 'Class 4': 0.9984242424242424}\nEpoch 2, Train Loss: 0.1748, Val Loss: 0.3239, Test Accuracy: 62.90%, F1-Weighted: 0.6046, F1-Macro: 0.7235\nPer-class F1-scores: {'Class 0': 0.6870843489004868, 'Class 1': 1.0, 'Class 2': 0.5153113499498161, 'Class 3': 0.416243654822335, 'Class 4': 0.9988280460699132}\nEpoch 3, Train Loss: 0.1569, Val Loss: 0.3195, Test Accuracy: 61.84%, F1-Weighted: 0.5895, F1-Macro: 0.6416\nPer-class F1-scores: {'Class 0': 0.6853247559731026, 'Class 1': 1.0, 'Class 2': 0.48824143361943834, 'Class 3': 0.035915304207987137, 'Class 4': 0.9986063140035146}\nEpoch 4, Train Loss: 0.1489, Val Loss: 0.3072, Test Accuracy: 62.26%, F1-Weighted: 0.5839, F1-Macro: 0.6408\nPer-class F1-scores: {'Class 0': 0.6986428083520231, 'Class 1': 1.0, 'Class 2': 0.4663212141965683, 'Class 3': 0.04231166150670794, 'Class 4': 0.9967400328021545}\nEpoch 5, Train Loss: 0.1438, Val Loss: 0.3084, Test Accuracy: 63.11%, F1-Weighted: 0.5988, F1-Macro: 0.6457\nPer-class F1-scores: {'Class 0': 0.7004267233990902, 'Class 1': 1.0, 'Class 2': 0.49300838224234955, 'Class 3': 0.03606470432246089, 'Class 4': 0.9988894049229651}\nEpoch 6, Train Loss: 0.1408, Val Loss: 0.3045, Test Accuracy: 62.44%, F1-Weighted: 0.5847, F1-Macro: 0.6395\nPer-class F1-scores: {'Class 0': 0.701146164275777, 'Class 1': 1.0, 'Class 2': 0.465536510811084, 'Class 3': 0.03387220939183987, 'Class 4': 0.9971885681923909}\nEpoch 7, Train Loss: 0.1382, Val Loss: 0.2978, Test Accuracy: 64.91%, F1-Weighted: 0.6237, F1-Macro: 0.6563\nPer-class F1-scores: {'Class 0': 0.709747473480954, 'Class 1': 1.0, 'Class 2': 0.5325288905059486, 'Class 3': 0.04194373401534527, 'Class 4': 0.9970649555694998}\nEpoch 8, Train Loss: 0.1355, Val Loss: 0.2946, Test Accuracy: 71.55%, F1-Weighted: 0.7143, F1-Macro: 0.6912\nPer-class F1-scores: {'Class 0': 0.7273903214027674, 'Class 1': 1.0, 'Class 2': 0.6899094176223899, 'Class 3': 0.04148747786491272, 'Class 4': 0.9974505281048925}\nEpoch 9, Train Loss: 0.1345, Val Loss: 0.2968, Test Accuracy: 65.87%, F1-Weighted: 0.6322, F1-Macro: 0.7158\nPer-class F1-scores: {'Class 0': 0.7167836872187644, 'Class 1': 1.0, 'Class 2': 0.5426552165135302, 'Class 3': 0.32233009708737864, 'Class 4': 0.9974302422047309}\nEpoch 10, Train Loss: 0.1325, Val Loss: 0.2915, Test Accuracy: 65.88%, F1-Weighted: 0.6393, F1-Macro: 0.6621\nPer-class F1-scores: {'Class 0': 0.7117330462863295, 'Class 1': 1.0, 'Class 2': 0.5604844373596155, 'Class 3': 0.04097951024487756, 'Class 4': 0.9974302422047309}\nEpoch 11, Train Loss: 0.1312, Val Loss: 0.2947, Test Accuracy: 67.40%, F1-Weighted: 0.6614, F1-Macro: 0.6836\nPer-class F1-scores: {'Class 0': 0.7131982117297804, 'Class 1': 1.0, 'Class 2': 0.6013270091143157, 'Class 3': 0.10635538261997406, 'Class 4': 0.9973693795782913}\nEpoch 12, Train Loss: 0.1287, Val Loss: 0.2727, Test Accuracy: 76.21%, F1-Weighted: 0.7606, F1-Macro: 0.7103\nPer-class F1-scores: {'Class 0': 0.7755377065521604, 'Class 1': 1.0, 'Class 2': 0.7367979390824843, 'Class 3': 0.04170905391658189, 'Class 4': 0.9973490903939939}\nEpoch 13, Train Loss: 0.1251, Val Loss: 0.2601, Test Accuracy: 81.29%, F1-Weighted: 0.8142, F1-Macro: 0.7318\nPer-class F1-scores: {'Class 0': 0.8102898346874948, 'Class 1': 1.0, 'Class 2': 0.8091620506319553, 'Class 3': 0.04205128205128206, 'Class 4': 0.9974708131841451}\nEpoch 14, Train Loss: 0.1216, Val Loss: 0.2465, Test Accuracy: 79.75%, F1-Weighted: 0.7978, F1-Macro: 0.7816\nPer-class F1-scores: {'Class 0': 0.7804125682110459, 'Class 1': 1.0, 'Class 2': 0.8034600039053278, 'Class 3': 0.32669322709163345, 'Class 4': 0.9973896679414801}\nEpoch 15, Train Loss: 0.1153, Val Loss: 0.2105, Test Accuracy: 89.83%, F1-Weighted: 0.8983, F1-Macro: 0.8213\nPer-class F1-scores: {'Class 0': 0.8968505988584643, 'Class 1': 1.0, 'Class 2': 0.8950852781605014, 'Class 3': 0.31721470019342357, 'Class 4': 0.9974505281048925}\nEpoch 16, Train Loss: 0.1014, Val Loss: 0.1275, Test Accuracy: 98.53%, F1-Weighted: 0.9870, F1-Macro: 0.8028\nPer-class F1-scores: {'Class 0': 0.9876487066817059, 'Class 1': 1.0, 'Class 2': 0.9862312329266244, 'Class 3': 0.042652795838751624, 'Class 4': 0.9974708131841451}\nEpoch 17, Train Loss: 0.0885, Val Loss: 0.0893, Test Accuracy: 99.29%, F1-Weighted: 0.9947, F1-Macro: 0.8044\nPer-class F1-scores: {'Class 0': 0.9963161408761231, 'Class 1': 1.0, 'Class 2': 0.9934123947607907, 'Class 3': 0.03490136570561457, 'Class 4': 0.9974115267947422}\nEpoch 18, Train Loss: 0.0814, Val Loss: 0.0688, Test Accuracy: 99.75%, F1-Weighted: 0.9976, F1-Macro: 0.8655\nPer-class F1-scores: {'Class 0': 0.9975831024236884, 'Class 1': 1.0, 'Class 2': 0.9978065734341531, 'Class 3': 0.33469387755102037, 'Class 4': 0.9975722262685118}\nEpoch 19, Train Loss: 0.0751, Val Loss: 0.0615, Test Accuracy: 99.54%, F1-Weighted: 0.9955, F1-Macro: 0.8624\nPer-class F1-scores: {'Class 0': 0.9952278228867986, 'Class 1': 1.0, 'Class 2': 0.9958071696356002, 'Class 3': 0.3234714003944773, 'Class 4': 0.9974505281048925}\nEpoch 20, Train Loss: 0.0725, Val Loss: 0.0520, Test Accuracy: 99.84%, F1-Weighted: 0.9985, F1-Macro: 0.8650\nPer-class F1-scores: {'Class 0': 0.9985911668610397, 'Class 1': 1.0, 'Class 2': 0.9986808604936264, 'Class 3': 0.33061224489795915, 'Class 4': 0.997105673285162}\nLoaded best model with validation loss: 0.05203283033662589\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"print(f\"Final Accuracy: {accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:12:03.267057Z","iopub.execute_input":"2025-04-05T10:12:03.267414Z","iopub.status.idle":"2025-04-05T10:12:03.271688Z","shell.execute_reply.started":"2025-04-05T10:12:03.267380Z","shell.execute_reply":"2025-04-05T10:12:03.270819Z"}},"outputs":[{"name":"stdout","text":"Final Accuracy: 99.84%\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:12:11.988001Z","iopub.execute_input":"2025-04-05T10:12:11.988409Z","iopub.status.idle":"2025-04-05T10:12:11.994878Z","shell.execute_reply.started":"2025-04-05T10:12:11.988375Z","shell.execute_reply":"2025-04-05T10:12:11.993841Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"SpinalSAENet(\n  (encoder): Sequential(\n    (0): Linear(in_features=11, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.6, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): ReLU()\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=64, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.6, inplace=False)\n    (3): Linear(in_features=32, out_features=5, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":86},{"cell_type":"markdown","source":"# confusion  matrix of the classifed samples","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(all_targets, all_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.835149Z","iopub.status.idle":"2025-04-05T09:50:16.835525Z","shell.execute_reply":"2025-04-05T09:50:16.835362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ROC","metadata":{}},{"cell_type":"code","source":"# ROC Curves\ny_test_bin = label_binarize(all_targets, classes=range(num_classes))\nall_scores = np.array(all_scores)\n\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], all_scores[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves (One-vs-Rest)\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.836884Z","iopub.status.idle":"2025-04-05T09:50:16.837233Z","shell.execute_reply":"2025-04-05T09:50:16.837079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Save the entire model (structure + weights)\nmodel.cpu()  # Move to CPU before pickling\nmodel.eval()\n\nwith open('intrusion_Model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nprint(\"Model saved as intrusion_Model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:24:35.677593Z","iopub.execute_input":"2025-04-05T10:24:35.677889Z","iopub.status.idle":"2025-04-05T10:24:35.684975Z","shell.execute_reply.started":"2025-04-05T10:24:35.677867Z","shell.execute_reply":"2025-04-05T10:24:35.684277Z"}},"outputs":[{"name":"stdout","text":"Model saved as intrusion_Model.pkl\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"with open('intrusion_Model.pkl', 'rb') as f:\n    loaded_model = pickle.load(f)\nloaded_model.eval()\nloaded_model.to(device)  # move back to GPU if needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:50:16.839632Z","iopub.status.idle":"2025-04-05T09:50:16.839994Z","shell.execute_reply":"2025-04-05T09:50:16.839834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# cusytom input prediciton","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport joblib\nfrom sklearn.preprocessing import StandardScaler\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# === Define the model architectur\n\n# === Class label mapping === #\nlabel_mapping = {\n    0: \"Dos\",\n    1: \"Theft\",\n    2: \"DDos\",\n    3: \"Normal\",\n    4: \"Reconnaissance\"\n}\n\n# === Required features in order === #\nrequired_features = [\n    'pkts', 'bytes', 'sbytes', 'dbytes', 'rate',\n    'mean', 'stddev', 'sum', 'proto', 'state',\n     'dur'\n]\n\n# === Load the trained model === #\ntry:\n    # Use same SpinalSAENet class definition as during training\n    model = SpinalSAENet(input_dim=11, hidden_dims=[128, 64], num_classes=5).to(device)\n    model.load_state_dict(torch.load(\"/kaggle/working/spinal_saenet_best.pth\", map_location=device))\n    model.eval()\n\n    print(\"Model loaded successfully.\")\nexcept Exception as e:\n    print(\"Error loading model:\", e)\n    raise\n\n# === Load the scaler === #\ntry:\n    scaler = joblib.load('/kaggle/working/scaler.pkl')\n    print(\"Scaler loaded successfully.\")\nexcept Exception as e:\n    print(\"Error loading scaler:\", e)\n    raise\n\n# === Direct custom input (your example) === #\ncustom_input = {\n    'pkts': 100,\n    'bytes': 5000,\n    'sbytes': 3000,\n    'dbytes': 2000,\n    'rate': 50.0,\n    'mean': 25.0,\n    'stddev': 10.0,\n    'sum': 10000,\n    'proto': 0,  # Encoded value (e.g., 0 for TCP)\n    'state': 1,  # Encoded value (e.g., 1 for CON)\n    # 'stime': 1620000000,\n    'dur': 0.5\n}\n\n# === Prediction pipeline === #\ntry:\n    # Convert input to array\n    input_array = np.array([[custom_input[feat] for feat in required_features]], dtype=np.float32)\n\n    # Scale the input\n    input_scaled = scaler.transform(input_array)\n\n    # Convert to torch tensor\n    X_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n\n    # Predict\n    with torch.no_grad():\n        outputs = model(X_tensor)\n        probs = torch.softmax(outputs, dim=1).cpu().numpy().flatten()\n        predicted_class = int(np.argmax(probs))\n        confidence = probs[predicted_class]\n\n    print(\"\\n=== Prediction Result ===\")\n    print(f\"Predicted Label: {label_mapping[predicted_class]}\")\n    print(f\"Confidence: {confidence:.4f}\")\n    print(\"Class Probabilities:\")\n    for idx, prob in enumerate(probs):\n        print(f\"  {label_mapping[idx]}: {prob:.4f}\")\n\nexcept Exception as e:\n    print(\"Prediction failed:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:24:03.478774Z","iopub.execute_input":"2025-04-05T10:24:03.479077Z","iopub.status.idle":"2025-04-05T10:24:03.497694Z","shell.execute_reply.started":"2025-04-05T10:24:03.479055Z","shell.execute_reply":"2025-04-05T10:24:03.496816Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel loaded successfully.\nScaler loaded successfully.\n\n=== Prediction Result ===\nPredicted Label: Normal\nConfidence: 1.0000\nClass Probabilities:\n  Dos: 0.0000\n  Theft: 0.0000\n  DDos: 0.0000\n  Normal: 1.0000\n  Reconnaissance: 0.0000\n","output_type":"stream"}],"execution_count":91}]}